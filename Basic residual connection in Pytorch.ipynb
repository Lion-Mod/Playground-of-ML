{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO+S/T9BAfLQWflSRqyjCpe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "41uHbAWUfwZc"
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuXT2i2Pi1di"
      },
      "source": [
        "# Prepare data and dataloader\n",
        "train = datasets.MNIST(\"data\", train = True, download = True, transform = transforms.ToTensor())\n",
        "train, valid = random_split(train, [55000, 5000]) # Perform data split\n",
        "train_dl = DataLoader(train, batch_size = 32) # Train dataloader\n",
        "valid_dl = DataLoader(valid, batch_size = 32) # Valid dataloader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7_eZB5Of0_g"
      },
      "source": [
        "# Create basic model\n",
        "basic_model = nn.Sequential(\n",
        "    nn.Linear(28 * 28, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.1),\n",
        "    nn.Linear(64, 10)\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6fVmbw_6qOb"
      },
      "source": [
        "# Flexy model\n",
        "class FlexyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(28 * 28, 64)\n",
        "    self.l2 = nn.Linear(64, 64)\n",
        "    self.l3 = nn.Linear(64, 10)\n",
        "    self.dropo = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h1 = nn.functional.relu(self.l1(x))\n",
        "    h2 = nn.functional.relu(self.l2(h1))\n",
        "\n",
        "    # Here is the residual connection\n",
        "    dropo = self.dropo(h2 + h1)\n",
        "\n",
        "    logits = self.l3(dropo)\n",
        "    return logits\n",
        "  \n",
        "flexy_model = FlexyModel()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTQnrRjM_qYx"
      },
      "source": [
        "def training(model):\n",
        "\n",
        "  # Create optimiser\n",
        "  optimiser = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "\n",
        "  # Create loss function CEL\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  n_epochs = 7\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    losses = list()\n",
        "    accuracies = list()\n",
        "    for batch in train_dl:\n",
        "      # x = image\n",
        "      x, y = batch\n",
        "\n",
        "      # Each batch is a 28 x 28 image * number of images\n",
        "      # i.e. number of images * 28 x 28\n",
        "      b = x.size(0)\n",
        "      x = x.view(b, -1)\n",
        "\n",
        "      # Forward step\n",
        "      logits = model(x)\n",
        "\n",
        "      # Objective function\n",
        "      # - calculate training loss using logits and actual\n",
        "      obj = loss(logits, y)\n",
        "\n",
        "      # Clean gradient\n",
        "      model.zero_grad()\n",
        "\n",
        "      # Accumulate partial derivs of obj w.r.t params\n",
        "      obj.backward()\n",
        "\n",
        "      # Updates! I.e. step opposite direction of grads\n",
        "      optimiser.step()\n",
        "\n",
        "      losses.append(obj.item())\n",
        "\n",
        "    print(\"training loss:\", torch.tensor(losses).mean())\n",
        "\n",
        "    # REPEAT FOR VALIDATION, not as many steps!\n",
        "    losses = list()\n",
        "    model.eval()\n",
        "    for batch in valid_dl:\n",
        "      x, y = batch\n",
        "\n",
        "      b = x.size(0)\n",
        "      x = x.view(b, -1)\n",
        "\n",
        "      # Note no_grad here (i.e. don't keep tracing gradients, graphs...)\n",
        "      with torch.no_grad():\n",
        "        logits = model(x)\n",
        "\n",
        "      # Calculate validation loss\n",
        "        obj = loss(logits, y)\n",
        "\n",
        "      losses.append(obj.item())\n",
        "\n",
        "    print(\"valid loss:\", torch.tensor(losses).mean())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbk64UEi_rcd",
        "outputId": "d0e9051a-5dd7-4a9c-fc9d-5b2d0d8662b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "training(basic_model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training loss: tensor(1.2649)\n",
            "valid loss: tensor(0.4831)\n",
            "training loss: tensor(0.3890)\n",
            "valid loss: tensor(0.3399)\n",
            "training loss: tensor(0.3155)\n",
            "valid loss: tensor(0.2927)\n",
            "training loss: tensor(0.2794)\n",
            "valid loss: tensor(0.2628)\n",
            "training loss: tensor(0.2531)\n",
            "valid loss: tensor(0.2410)\n",
            "training loss: tensor(0.2321)\n",
            "valid loss: tensor(0.2231)\n",
            "training loss: tensor(0.2145)\n",
            "valid loss: tensor(0.2083)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euKBLhDpCSIx",
        "outputId": "37e80e50-be51-414d-ef14-5bedf9f8e647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "training(flexy_model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training loss: tensor(0.8328)\n",
            "valid loss: tensor(0.3870)\n",
            "training loss: tensor(0.3370)\n",
            "valid loss: tensor(0.3029)\n",
            "training loss: tensor(0.2841)\n",
            "valid loss: tensor(0.2634)\n",
            "training loss: tensor(0.2498)\n",
            "valid loss: tensor(0.2343)\n",
            "training loss: tensor(0.2228)\n",
            "valid loss: tensor(0.2110)\n",
            "training loss: tensor(0.2009)\n",
            "valid loss: tensor(0.1920)\n",
            "training loss: tensor(0.1829)\n",
            "valid loss: tensor(0.1766)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}